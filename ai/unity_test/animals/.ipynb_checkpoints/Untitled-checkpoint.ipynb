{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import newaxis\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, env, n_actions, n_features, action_low=-1, action_high=1, reward_decay=0.99,\n",
    "                 actor_learning_rate=0.01, critic_learning_rate=0.01, learning_rate_decay=0.95,\n",
    "                 ):\n",
    "        self.env = env\n",
    "        self.state_size = n_features\n",
    "        self.action_size = n_actions\n",
    "        self.action_low = action_low\n",
    "        self.action_high = action_high\n",
    "        self.gamma = reward_decay   # discount rate\n",
    "        self.actor_model_set = True\n",
    "        self.critic_model_set = True\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.critic_learning_rate = critic_learning_rate # often larger than actor_learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.batch_size = 64\n",
    "        self.epsilon = 0.2 # used to clip\n",
    "        self.entfact = 2e-2 # entropy factor, to encourage exploration\n",
    "        self.lam = 0.95 # gae factor\n",
    "        self.memory = [] # store (s, a, r) for one agent\n",
    "        self.agents = 5 # number of agents that collect memory\n",
    "        self.history = {} # store the memory for different agents\n",
    "        self.history['states'] = []\n",
    "        self.history['observations'] = []\n",
    "        self.history['actions'] = []\n",
    "        self.history['discounted_rs'] = []\n",
    "        self.history['advantages'] = []\n",
    "        self._construct_nets()\n",
    "        \n",
    "    def _construct_nets(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.tfs = tf.placeholder(tf.float32, [None, self.state_size], 'state')\n",
    "        self.obs = tf.placeholder(tf.float32, [None, 16, 40, 1], 'observation')\n",
    "\n",
    "        # critic\n",
    "        with tf.variable_scope('critic'):\n",
    "#             net = tf.layers.conv2d(self.obs,\n",
    "#                                    filters=10,\n",
    "#                                    kernel_size=[5, 5],\n",
    "#                                    padding=\"same\",\n",
    "#                                    activation=tf.nn.relu)\n",
    "#             net = tf.nn.relu(tf.layers.batch_normalization(net))\n",
    "#             net = tf.layers.max_pooling2d(net, pool_size=[2, 2], strides=2)\n",
    "#             net = tf.layers.conv2d(net,\n",
    "#                                    filters=32,\n",
    "#                                    kernel_size=[5, 5],\n",
    "#                                    padding=\"same\",\n",
    "#                                    activation=tf.nn.relu)\n",
    "#             net = tf.layers.max_pooling2d(net, pool_size=[3, 3], strides=3)\n",
    "            n_l1_c = 128\n",
    "            net = tf.reshape(self.obs, [-1, 16*40])\n",
    "            w1_c_ob = tf.get_variable('w1_c_ob', [16*40, n_l1_c])\n",
    "            w1_c_s = tf.get_variable('w1_c_s', [self.state_size, n_l1_c])\n",
    "            b1_c = tf.get_variable('b1_c', [1, n_l1_c])\n",
    "            net = tf.nn.relu(tf.matmul(net, w1_c_ob) + tf.matmul(self.tfs, w1_c_s) + b1_c)\n",
    "            net = tf.layers.dense(net, 32, tf.nn.relu)\n",
    "            self.v = tf.layers.dense(net, 1)\n",
    "            self.tfdc_r = tf.placeholder(tf.float32, [None, 1], 'discounted_r')\n",
    "            self.closs = tf.reduce_mean(tf.square(self.tfdc_r - self.v))\n",
    "            self.ctrain_op = tf.train.AdamOptimizer(self.critic_learning_rate).minimize(self.closs)\n",
    "\n",
    "        # actor\n",
    "        pi, pi_params = self._build_anet('pi', trainable=True)\n",
    "        test_pi = tf.distributions.Normal(loc=pi.mean(), scale=tf.zeros_like(pi.stddev()))\n",
    "        oldpi, oldpi_params = self._build_anet('oldpi', trainable=False)\n",
    "        with tf.variable_scope('sample_action'):\n",
    "            self.sample_op = tf.squeeze(pi.sample(1), axis=0)       # choosing action\n",
    "            self.sample_test = tf.squeeze(test_pi.sample(1), axis=0) # deterministic action in test\n",
    "        with tf.variable_scope('update_oldpi'):\n",
    "            self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n",
    "\n",
    "        self.tfa = tf.placeholder(tf.float32, [None, self.action_size], 'action')\n",
    "        self.tfadv = tf.placeholder(tf.float32, [None, 1], 'advantage')\n",
    "        with tf.variable_scope('loss'):\n",
    "            with tf.variable_scope('surrogate'):\n",
    "                self.ratio = pi.prob(self.tfa) / (oldpi.prob(self.tfa)+1e-10)\n",
    "                surr = self.ratio * self.tfadv\n",
    "                surr2 = tf.clip_by_value(self.ratio, 1-self.epsilon, 1+self.epsilon) * self.tfadv\n",
    "                self.aloss = - tf.reduce_mean(tf.minimum(surr, surr2)) - self.entfact * tf.reduce_mean(pi.entropy())\n",
    "\n",
    "        with tf.variable_scope('atrain'):\n",
    "            self.atrain_op = tf.train.AdamOptimizer(self.actor_learning_rate).minimize(self.aloss, var_list=pi_params)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build_anet(self, name, trainable):\n",
    "        with tf.variable_scope(name):\n",
    "#             net = tf.layers.conv2d(self.obs,\n",
    "#                                    filters=5,\n",
    "#                                    kernel_size=[3, 3],\n",
    "#                                    padding=\"same\",\n",
    "#                                    activation=tf.nn.relu,\n",
    "#                                    trainable=trainable)\n",
    "#             net = tf.layers.max_pooling2d(net, pool_size=[2, 2], strides=2)\n",
    "#             net = tf.layers.conv2d(net,\n",
    "#                                    filters=32,\n",
    "#                                    kernel_size=[5, 5],\n",
    "#                                    padding=\"same\",\n",
    "#                                    activation=tf.nn.relu,\n",
    "#                                    trainable=trainable)\n",
    "#             net = tf.layers.max_pooling2d(net, pool_size=[3, 3], strides=3)\n",
    "            n_l1_a = 128\n",
    "            net = tf.reshape(self.obs, [-1, 16*40])\n",
    "            w1_a_ob = tf.get_variable('w1_a_ob', [16*40, n_l1_a], trainable=trainable)\n",
    "            w1_a_s = tf.get_variable('w1_a_s', [self.state_size, n_l1_a], trainable=trainable)\n",
    "            b1_a = tf.get_variable('b1_a', [1, n_l1_a], trainable=trainable)\n",
    "            net = tf.nn.relu(tf.matmul(net, w1_a_ob) + tf.matmul(self.tfs, w1_a_s) + b1_a)\n",
    "            net = tf.layers.dense(net, 32, tf.nn.relu, trainable=trainable)\n",
    "#             net = tf.layers.dense(net, 512, trainable=trainable)\n",
    "#             net = tf.nn.relu(tf.layers.batch_normalization(net))\n",
    "            mu = max(np.abs(self.action_low), np.abs(self.action_high)) * tf.layers.dense(net, self.action_size, tf.nn.tanh, trainable=trainable)\n",
    "            sigma = tf.layers.dense(net, self.action_size, tf.nn.softplus, trainable=trainable)\n",
    "            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma)\n",
    "        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n",
    "        return norm_dist, params\n",
    "    \n",
    "    def choose_action(self, state, observation, train=True): # normal distribution\n",
    "        assert self.actor_model_set, 'actor model not set!'\n",
    "        if train:\n",
    "            a = self.sess.run(self.sample_op, {self.tfs: state, self.obs: observation})[0]\n",
    "        else:\n",
    "            a = self.sess.run(self.sample_test, {self.tfs: state, self.obs: observation})[0]\n",
    "        return np.clip(a, self.action_low, self.action_high)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory += [[state[0], action, reward, next_state[0]]]\n",
    "    \n",
    "    def discount_rewards(self, rewards, gamma, value_next=0.0):\n",
    "        discounted_r = np.zeros_like(rewards)\n",
    "        running_add = value_next\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            discounted_r[t] = running_add = running_add * gamma + rewards[t]\n",
    "        return discounted_r\n",
    "    \n",
    "    def process_memory(self):\n",
    "        memory = np.vstack(self.memory)\n",
    "        states = np.vstack(memory[:,0])\n",
    "        actions = np.vstack(memory[:,1])\n",
    "        rewards = memory[:,2]\n",
    "        last_next_state = memory[:,3][-1]\n",
    "        discounted_ep_rs = self.discount_rewards(rewards, self.gamma)[:, newaxis]\n",
    "#         value_estimates = self.sess.run(self.v, {self.tfs: states, self.obs: observations}).flatten()\n",
    "        value_estimates = self.sess.run(self.v, {self.tfs: np.r_[states, last_next_state[newaxis, :]]}).flatten()\n",
    "#         last_value_estimate = self.sess.run(self.v, {self.tfs: , self.obs: })[0]\n",
    "#         value_estimates = np.append(value_estimates, 0)\n",
    "        delta_t = rewards + self.gamma * value_estimates[1:] - value_estimates[:-1]\n",
    "        advs = self.discount_rewards(delta_t, self.gamma * self.lam)[:, newaxis] #gae\n",
    "        last = states.shape[0]\n",
    "        self.history['states'] += [states[-last:]]\n",
    "        self.history['actions'] += [actions[-last:]]\n",
    "        self.history['discounted_rs'] += [discounted_ep_rs[-last:]]\n",
    "        self.history['advantages'] += [advs[-last:]]\n",
    "        self.memory = [] # empty the memory\n",
    "    \n",
    "    def replay(self):\n",
    "        assert self.actor_model_set, 'model not set!'\n",
    "        assert self.critic_model_set, 'critic model not set!'\n",
    "        self.sess.run(self.update_oldpi_op)\n",
    "        \n",
    "        s = np.vstack(self.history['states'])\n",
    "        ac = np.vstack(self.history['actions'])\n",
    "        dc_r = np.vstack(self.history['discounted_rs'])\n",
    "        ad = np.vstack(self.history['advantages'])\n",
    "#         ad = (ad-ad.mean())/ad.std()\n",
    "        \n",
    "        for _ in range(10): # update K epochs\n",
    "            s, ac, dc_r, ad = shuffle(s, ac, dc_r, ad)\n",
    "            for l in range(s.shape[0]//self.batch_size):\n",
    "                start = l * self.batch_size\n",
    "                end = (l + 1) * self.batch_size\n",
    "                self.sess.run(self.atrain_op, {self.tfs: s[start:end], self.tfa: ac[start:end], self.tfadv: ad[start:end]})\n",
    "                self.sess.run(self.ctrain_op, {self.tfs: s[start:end], self.tfdc_r: dc_r[start:end]})\n",
    "            if s.shape[0] % self.batch_size != 0:\n",
    "                res = s.shape[0] % self.batch_size\n",
    "                self.sess.run(self.atrain_op, {self.tfs: s[-res:], self.tfa: ac[-res:], self.tfadv: ad[-res:]})\n",
    "                self.sess.run(self.ctrain_op, {self.tfs: s[-res:], self.tfdc_r: dc_r[-res:]})\n",
    "#         self.actor_learning_rate *= self.learning_rate_decay\n",
    "#         self.critic_learning_rate *= self.learning_rate_decay\n",
    "        \n",
    "        for key in self.history:\n",
    "            self.history[key] = [] # empty the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents.environment:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 4\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 2\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , \n",
      "Agent state looks like: \n",
      "[ 0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "env_name = \"bipedal\"\n",
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]\n",
    "\n",
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=False)[default_brain]\n",
    "    \n",
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.states[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
