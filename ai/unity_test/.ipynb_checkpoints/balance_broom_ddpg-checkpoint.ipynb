{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "class Memory:   # stored as ( s, a, r, s_ ) in SumTree\n",
    "\n",
    "    def __init__(self, capacity, e = 0.01, a = 0.6):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "        self.size = 0\n",
    "        self.e = e # error\n",
    "        self.a = a # priority exponent, 0 = no priority\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, sample, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample) \n",
    "        self.size += 1\n",
    "        if self.size > self.capacity:\n",
    "            self.size = self.capacity\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        segment = self.tree.total() / n\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, _, data) = self.tree.get(s)\n",
    "            batch.append( (idx, data) )\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn.preprocessing\n",
    "import sklearn.pipeline\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, env, n_actions, n_features, action_low, action_high, featurize=False, reward_decay=0.95,\n",
    "                 actor_learning_rate=0.01, critic_learning_rate=0.01, learning_rate_decay=0.95,\n",
    "                 memory_size=10000, priority_alpha=0.6, tau=0.9, variance=3):\n",
    "        self.env = env\n",
    "        self.state_size = n_features\n",
    "        self.action_size = n_actions\n",
    "        self.action_low = action_low\n",
    "        self.action_high = action_high\n",
    "        self.gamma = reward_decay   # discount rate\n",
    "        self.actor_model_set = False\n",
    "        self.critic_model_set = False\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.critic_learning_rate = critic_learning_rate # often larger than actor_learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.priority_alpha = priority_alpha\n",
    "        self.tau = tau # soft update\n",
    "        self.batch_size = 32\n",
    "        self.memory = Memory(capacity=memory_size, a=priority_alpha)\n",
    "        self.variance = variance # exploration\n",
    "        self.memory_size = memory_size\n",
    "        self.featurize = featurize\n",
    "        if featurize:\n",
    "            self._init_featurizer()\n",
    "        self._construct_nets()\n",
    "        \n",
    "    def _construct_nets(self):\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.S = tf.placeholder(tf.float32, [None, self.state_size], 'state')\n",
    "        self.S_ = tf.placeholder(tf.float32, [None, self.state_size], 'next_state')\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1], 'r')\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.a = self._build_a(self.S, scope='eval', trainable=True)\n",
    "            self.a_ = self._build_a(self.S_, scope='target', trainable=False)\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # assign self.a = a in memory when calculating q for td_error,\n",
    "            # otherwise the self.a is from Actor when updating Actor\n",
    "            q = self._build_c(self.S, self.a, scope='eval', trainable=True)\n",
    "            q_ = self._build_c(self.S_, self.a_, scope='target', trainable=False)\n",
    "        \n",
    "        # networks parameters\n",
    "        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        # target net replacement\n",
    "        self.soft_replace = [[tf.assign(ta, (1 - self.tau) * ta + self.tau * ea), tf.assign(tc, (1 - self.tau) * tc + self.tau * ec)]\n",
    "                             for ta, ea, tc, ec in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]\n",
    "\n",
    "        q_target = self.R + self.gamma * q_\n",
    "        # in the feed_dic for the td_error, the self.a should change to actions in memory\n",
    "        self.td_error_element_wise = tf.squared_difference(q_target, q)\n",
    "        self.td_error = tf.losses.mean_squared_error(labels=q_target, predictions=q)\n",
    "        self.ctrain = tf.train.AdamOptimizer(self.critic_learning_rate).minimize(self.td_error, var_list=self.ce_params)\n",
    "           \n",
    "        a_loss = - tf.reduce_mean(q)    # maximize the q\n",
    "        self.atrain = tf.train.AdamOptimizer(self.actor_learning_rate).minimize(a_loss, var_list=self.ae_params)\n",
    "        \n",
    "        self.saver = tf.train.Saver() # saver\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _build_a(self, s, scope, trainable): # policy\n",
    "        with tf.variable_scope(scope):\n",
    "            net = tf.layers.dense(s, 200, activation=tf.nn.relu, name='l1', trainable=trainable)\n",
    "            net = tf.layers.dense(net, 100, activation=tf.nn.relu, name='l2', trainable=trainable)\n",
    "            net = tf.layers.dense(net, 30, activation=tf.nn.relu, name='l3', trainable=trainable)\n",
    "            a = tf.layers.dense(net, self.action_size, activation=tf.nn.tanh, name='a', trainable=trainable)\n",
    "            self.actor_model_set = True\n",
    "            return a * (self.action_high-self.action_low)/2 + (self.action_high+self.action_low)/2\n",
    "    \n",
    "    def _build_c(self, s, a, scope, trainable): # advantage value\n",
    "        with tf.variable_scope(scope):\n",
    "            n_l1 = 200\n",
    "            w1_s = tf.get_variable('w1_s', [self.state_size, n_l1], trainable=trainable)\n",
    "            w1_a = tf.get_variable('w1_a', [self.action_size, n_l1], trainable=trainable)\n",
    "            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)\n",
    "            net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "            net = tf.layers.dense(net, 100, activation=tf.nn.relu, name='l2', trainable=trainable)\n",
    "            net = tf.layers.dense(net, 30, activation=tf.nn.relu, name='l3', trainable=trainable)\n",
    "            self.critic_model_set = True\n",
    "            return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if self.priority_alpha > 0: # prioritised\n",
    "            self.memory.add((state, action, reward, next_state, done), \n",
    "                            self.error(state, action, reward, next_state))\n",
    "        else: # non prioritised, every memory has priority 1\n",
    "            self.memory.add((state, action, reward, next_state, done), 1)\n",
    "            \n",
    "    def error(self, state, action, reward, next_state):\n",
    "        return self.sess.run(self.td_error, {self.S: state, self.a: [action], \n",
    "                                             self.R: [[reward]], self.S_: next_state})\n",
    "        \n",
    "    def choose_action(self, state, variance, low, high): # normal distribution\n",
    "        assert self.actor_model_set, 'actor model not set!'\n",
    "        action = self.sess.run(self.a, {self.S: state})[0]\n",
    "        return np.clip(np.random.normal(action, variance), low, high)\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        assert self.actor_model_set, 'model not set!'\n",
    "        assert self.critic_model_set, 'critic model not set!'\n",
    "        minibatch = self.memory.sample(batch_size)\n",
    "        idxs, states, actions, rewards, next_states = [], [], [], [], []\n",
    "        for idx, (state, action, reward, next_state, _) in minibatch:\n",
    "            idxs+=[idx]\n",
    "            states+=[state]\n",
    "            actions+=[action]\n",
    "            rewards+=[reward]\n",
    "            next_states+=[next_state]\n",
    "        \n",
    "        self.sess.run(self.atrain, {self.S: np.vstack(states)})\n",
    "        self.sess.run(self.ctrain, {self.S: np.vstack(states), self.a: np.vstack(actions),\n",
    "                                    self.R: np.vstack(rewards), self.S_: np.vstack(next_states)})\n",
    "        self.sess.run(self.soft_replace) # update the weights\n",
    "        \n",
    "        if self.priority_alpha > 0: # prioritised, update\n",
    "            errors = self.sess.run(self.td_error_element_wise, {self.S: np.vstack(states), self.a: np.vstack(actions),\n",
    "                                                                self.R: np.vstack(rewards), self.S_: np.vstack(next_states)})\n",
    "            for i in range(len(idxs)):\n",
    "                self.memory.update(idxs[i], errors[i])\n",
    "        \n",
    "        self.actor_learning_rate *= self.learning_rate_decay\n",
    "        self.critic_learning_rate *= self.learning_rate_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import sklearn.preprocessing\n",
    "import sklearn.pipeline\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from numpy import newaxis\n",
    "\n",
    "METHOD = [\n",
    "    dict(name='kl_pen', kl_target=0.01, lam=0.5),   # KL penalty\n",
    "    dict(name='clip', epsilon=0.2),                 # Clipped surrogate objective, find this is better\n",
    "][1]        # choose the method for optimization\n",
    "\n",
    "class PPOTest:\n",
    "    def __init__(self, env:gym.Env, n_actions, n_features, action_low, action_high, featurize=False, reward_decay=0.95,\n",
    "                 actor_learning_rate=0.01, critic_learning_rate=0.01, learning_rate_decay=0.9995,\n",
    "                 tau=1.0):\n",
    "        self.env = env\n",
    "        self.state_size = n_features\n",
    "        self.action_size = n_actions\n",
    "        self.action_low = action_low\n",
    "        self.action_high = action_high\n",
    "        self.gamma = reward_decay   # discount rate\n",
    "        self.actor_model_set = True\n",
    "        self.critic_model_set = True\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.critic_learning_rate = critic_learning_rate # often larger than actor_learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.tau = tau # soft update\n",
    "        self.batch_size = 32\n",
    "        self.memory = [] # store (s, a, r)\n",
    "        self.featurize = featurize\n",
    "        if featurize:\n",
    "            self._init_featurizer()\n",
    "        self._construct_nets()\n",
    "        \n",
    "    def _construct_nets(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.tfs = tf.placeholder(tf.float32, [None, self.state_size], 'state')\n",
    "\n",
    "        # critic\n",
    "        with tf.variable_scope('critic'):\n",
    "            net = tf.layers.dense(self.tfs, 400, tf.nn.relu)\n",
    "            net = tf.layers.dense(net, 200, tf.nn.relu)\n",
    "#             l3 = tf.layers.dense(l2, 30, tf.nn.relu)\n",
    "            self.v = tf.layers.dense(net, 1)\n",
    "            self.tfdc_r = tf.placeholder(tf.float32, [None, 1], 'discounted_r')\n",
    "            self.advantage = self.tfdc_r - self.v\n",
    "            self.closs = tf.reduce_mean(tf.square(self.advantage))\n",
    "            self.ctrain_op = tf.train.AdamOptimizer(self.critic_learning_rate).minimize(self.closs)\n",
    "\n",
    "        # actor\n",
    "        pi, pi_params = self._build_anet('pi', trainable=True)\n",
    "        oldpi, oldpi_params = self._build_anet('oldpi', trainable=False)\n",
    "        with tf.variable_scope('sample_action'):\n",
    "            self.sample_op = tf.squeeze(pi.sample(1), axis=0)       # choosing action\n",
    "        with tf.variable_scope('update_oldpi'):\n",
    "            self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n",
    "\n",
    "        self.tfa = tf.placeholder(tf.float32, [None, self.action_size], 'action')\n",
    "        self.tfadv = tf.placeholder(tf.float32, [None, 1], 'advantage')\n",
    "        with tf.variable_scope('loss'):\n",
    "            with tf.variable_scope('surrogate'):\n",
    "#                 ratio = tf.exp(pi.log_prob(self.tfa) - oldpi.log_prob(self.tfa))\n",
    "                ratio = pi.prob(self.tfa) / (oldpi.prob(self.tfa)+1e-10)\n",
    "                surr = ratio * self.tfadv\n",
    "            if METHOD['name'] == 'kl_pen':\n",
    "                self.tflam = tf.placeholder(tf.float32, None, 'lambda')\n",
    "                kl = tf.distributions.kl_divergence(oldpi, pi)\n",
    "                self.kl_mean = tf.reduce_mean(kl)\n",
    "                self.aloss = -(tf.reduce_mean(surr - self.tflam * kl))\n",
    "            else:   # clipping method, find this is better\n",
    "                self.aloss = -tf.reduce_mean(tf.minimum(\n",
    "                    surr,\n",
    "                    tf.clip_by_value(ratio, 1.-METHOD['epsilon'], 1.+METHOD['epsilon'])*self.tfadv))\n",
    "\n",
    "        with tf.variable_scope('atrain'):\n",
    "            self.atrain_op = tf.train.AdamOptimizer(self.actor_learning_rate).minimize(self.aloss)\n",
    "            \n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build_anet(self, name, trainable):\n",
    "        with tf.variable_scope(name):\n",
    "            net = tf.layers.dense(self.tfs, 400, tf.nn.relu, trainable=trainable)\n",
    "            net = tf.layers.dense(net, 200, tf.nn.relu, trainable=trainable)\n",
    "            net = tf.layers.dense(net, 100, tf.nn.relu, trainable=trainable)\n",
    "            mu = max(np.abs(self.action_low), np.abs(self.action_high)) * tf.layers.dense(net, self.action_size, tf.nn.tanh, trainable=trainable)\n",
    "            sigma = tf.layers.dense(net, self.action_size, tf.nn.softplus, trainable=trainable)\n",
    "            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma+1e-5)\n",
    "        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n",
    "        return norm_dist, params\n",
    "    \n",
    "    def choose_action(self, state): # normal distribution\n",
    "        assert self.actor_model_set, 'actor model not set!'\n",
    "        a = self.sess.run(self.sample_op, {self.tfs: state})[0]\n",
    "        return np.clip(a, self.action_low, self.action_high)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory += [[state[0], action, reward, next_state[0]]]\n",
    "    \n",
    "    def replay(self):\n",
    "        assert self.actor_model_set, 'model not set!'\n",
    "        assert self.critic_model_set, 'critic model not set!'\n",
    "        memory = np.vstack(self.memory)\n",
    "        states = np.vstack(memory[:,0])\n",
    "        actions = np.vstack(memory[:,1])\n",
    "        rewards = memory[:,2]\n",
    "        last_next_state = memory[:,3][-1]\n",
    "        \n",
    "        discounted_ep_rs = np.zeros_like(rewards)\n",
    "        running_add = self.sess.run(self.v, {self.tfs: [last_next_state]})[0]\n",
    "        for t in reversed(range(0, len(memory))):\n",
    "            running_add = running_add * self.gamma + rewards[t]\n",
    "            discounted_ep_rs[t] = running_add\n",
    "        \n",
    "        self.sess.run(self.update_oldpi_op)\n",
    "        adv = self.sess.run(self.advantage, {self.tfs: states, self.tfdc_r: discounted_ep_rs[:, newaxis]})\n",
    "        [self.sess.run(self.atrain_op, {self.tfs: states, self.tfa: actions, self.tfadv: adv}) for _ in range(10)]\n",
    "        [self.sess.run(self.ctrain_op, {self.tfs: states, self.tfdc_r: discounted_ep_rs[:, newaxis]}) for _ in range(10)]\n",
    "        \n",
    "        self.actor_learning_rate *= self.learning_rate_decay\n",
    "        self.critic_learning_rate *= self.learning_rate_decay\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 8\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 2\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "env_name = \"balance_broom\"\n",
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state looks like: \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "train_mode = False\n",
    "    \n",
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "    \n",
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.states[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = DDPGAgent(env,\n",
    "            n_actions=2,\n",
    "            n_features=8,\n",
    "            featurize=False, \n",
    "            action_high=1,\n",
    "            action_low=-1,\n",
    "            actor_learning_rate=0.0001,\n",
    "            critic_learning_rate=0.0002,\n",
    "            priority_alpha=0\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOTest(env,\n",
    "                n_actions=2,\n",
    "                n_features=8,\n",
    "                featurize=False, \n",
    "                action_high=1,\n",
    "                action_low=-1,\n",
    "                actor_learning_rate=0.0001,\n",
    "                critic_learning_rate=0.0002\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2000 rewards: -56.00 explore var: 1.452 rewards: -68.00 explore var: 3.00 70 rewards: -33.00 explore var: 3.00rewards: -46.00 explore var: 3.00\n",
      "\n",
      "finished learning!\n"
     ]
    }
   ],
   "source": [
    "#DDPG\n",
    "n_episodes = 2000\n",
    "\n",
    "# agent.saver.restore(agent.sess, \"model/model.ckpt\")\n",
    "rewards = []\n",
    "for i_episode in range(n_episodes):\n",
    "    env_info = agent.env.reset(train_mode=True)[default_brain]\n",
    "    state = env_info.states\n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action(state, agent.variance, agent.action_low, agent.action_high)\n",
    "        env_info = agent.env.step(action)[default_brain]\n",
    "        next_state = env_info.states\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        r += reward\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        # learn when memory is full, every BATCH steps\n",
    "        if len(agent.memory) == agent.memory_size:\n",
    "            agent.variance *= 0.999995\n",
    "            agent.replay(agent.batch_size)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, \"explore var: %.2f\" % agent.variance, end=\"\\r\")\n",
    "            rewards += [r]\n",
    "            break\n",
    "print(\"\\n\")\n",
    "print(\"finished learning!\")\n",
    "agent.saver.save(agent.sess, \"model/model3g_ddpg.ckpt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 140 rewards: 69.1557ewards: 84.30 60 rewards: 66.85 88 rewards: 71.24\r"
     ]
    }
   ],
   "source": [
    "# PPO\n",
    "n_episodes = 2000\n",
    "\n",
    "# agent.saver.restore(agent.sess, \"model/model3g_ppo3.ckpt\")\n",
    "rewards = []\n",
    "for i_episode in range(n_episodes):\n",
    "    env_info = agent.env.reset(train_mode=True)[default_brain]\n",
    "    state = env_info.states\n",
    "    r = 0\n",
    "    count = 0\n",
    "    while count<4500:\n",
    "        action = agent.choose_action(state)\n",
    "        env_info = agent.env.step(action)[default_brain]\n",
    "        next_state = env_info.states\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        r += reward\n",
    "        agent.remember(state, action, reward, next_state)\n",
    "        # learn when memory is full, every BATCH steps\n",
    "        if len(agent.memory) == agent.batch_size or done:\n",
    "            agent.replay()\n",
    "        state = next_state\n",
    "        count+=1\n",
    "        if done or count==4500:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            rewards += [r]\n",
    "            break\n",
    "print(\"\\n\")\n",
    "print(\"finished learning!\")\n",
    "agent.saver.save(agent.sess, \"model/model3g_ppo2.ckpt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1923c8e9a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 20 rewards: -20.00\n",
      "\n",
      "finished testing!\n"
     ]
    }
   ],
   "source": [
    "#ddpg\n",
    "n_episodes = 10\n",
    "\n",
    "test_rewards = []\n",
    "# agent.saver.restore(agent.sess, \"model/model3g.ckpt\")\n",
    "for i_episode in range(n_episodes):\n",
    "    env_info = agent.env.reset(train_mode=False)[default_brain]\n",
    "    state = env_info.states\n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action(state, 0, agent.action_low, agent.action_high)\n",
    "        env_info = agent.env.step(action)[default_brain]\n",
    "        next_state = env_info.states\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        r += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            test_rewards += [r]\n",
    "            break\n",
    "print(\"\\n\")\n",
    "print(\"finished testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10 rewards: 205.65\n",
      "\n",
      "finished testing!\n"
     ]
    }
   ],
   "source": [
    "#ppo\n",
    "n_episodes = 10\n",
    "\n",
    "test_rewards = []\n",
    "# agent.saver.restore(agent.sess, \"model/model3g_ppo3.ckpt\")\n",
    "for i_episode in range(n_episodes):\n",
    "    env_info = agent.env.reset(train_mode=False)[default_brain]\n",
    "    state = env_info.states\n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action(state)\n",
    "        env_info = agent.env.step(action)[default_brain]\n",
    "        next_state = env_info.states\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        r += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            test_rewards += [r]\n",
    "            break\n",
    "print(\"\\n\")\n",
    "print(\"finished testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDX+Dzd7Jv6ajfm4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x245790109b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265.83999999999997"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rewards[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.519999999999996"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rewards[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
