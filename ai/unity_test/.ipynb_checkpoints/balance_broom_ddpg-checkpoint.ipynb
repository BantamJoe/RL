{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "class Memory:   # stored as ( s, a, r, s_ ) in SumTree\n",
    "\n",
    "    def __init__(self, capacity, e = 0.01, a = 0.6):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "        self.size = 0\n",
    "        self.e = e # error\n",
    "        self.a = a # priority exponent, 0 = no priority\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, sample, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample) \n",
    "        self.size += 1\n",
    "        if self.size > self.capacity:\n",
    "            self.size = self.capacity\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        segment = self.tree.total() / n\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, _, data) = self.tree.get(s)\n",
    "            batch.append( (idx, data) )\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn.preprocessing\n",
    "import sklearn.pipeline\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, env, n_actions, n_features, action_low, action_high, featurize=False, reward_decay=0.95,\n",
    "                 actor_learning_rate=0.01, critic_learning_rate=0.01, learning_rate_decay=0.95,\n",
    "                 memory_size=10000, priority_alpha=0.6, tau=0.9, variance=3):\n",
    "        self.env = env\n",
    "        self.state_size = n_features\n",
    "        self.action_size = n_actions\n",
    "        self.action_low = action_low\n",
    "        self.action_high = action_high\n",
    "        self.gamma = reward_decay   # discount rate\n",
    "        self.actor_model_set = False\n",
    "        self.critic_model_set = False\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.critic_learning_rate = critic_learning_rate # often larger than actor_learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.priority_alpha = priority_alpha\n",
    "        self.tau = tau # soft update\n",
    "        self.batch_size = 32\n",
    "        self.memory = Memory(capacity=memory_size, a=priority_alpha)\n",
    "        self.variance = variance # exploration\n",
    "        self.memory_size = memory_size\n",
    "        self.featurize = featurize\n",
    "        if featurize:\n",
    "            self._init_featurizer()\n",
    "        self._construct_nets()\n",
    "        \n",
    "    def _construct_nets(self):\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.S = tf.placeholder(tf.float32, [None, self.state_size], 'state')\n",
    "        self.S_ = tf.placeholder(tf.float32, [None, self.state_size], 'next_state')\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1], 'r')\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.a = self._build_a(self.S, scope='eval', trainable=True)\n",
    "            self.a_ = self._build_a(self.S_, scope='target', trainable=False)\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # assign self.a = a in memory when calculating q for td_error,\n",
    "            # otherwise the self.a is from Actor when updating Actor\n",
    "            q = self._build_c(self.S, self.a, scope='eval', trainable=True)\n",
    "            q_ = self._build_c(self.S_, self.a_, scope='target', trainable=False)\n",
    "        \n",
    "        # networks parameters\n",
    "        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        # target net replacement\n",
    "        self.soft_replace = [[tf.assign(ta, (1 - self.tau) * ta + self.tau * ea), tf.assign(tc, (1 - self.tau) * tc + self.tau * ec)]\n",
    "                             for ta, ea, tc, ec in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]\n",
    "\n",
    "        q_target = self.R + self.gamma * q_\n",
    "        # in the feed_dic for the td_error, the self.a should change to actions in memory\n",
    "        self.td_error_element_wise = tf.squared_difference(q_target, q)\n",
    "        self.td_error = tf.losses.mean_squared_error(labels=q_target, predictions=q)\n",
    "        self.ctrain = tf.train.AdamOptimizer(self.critic_learning_rate).minimize(self.td_error, var_list=self.ce_params)\n",
    "           \n",
    "        a_loss = - tf.reduce_mean(q)    # maximize the q\n",
    "        self.atrain = tf.train.AdamOptimizer(self.actor_learning_rate).minimize(a_loss, var_list=self.ae_params)\n",
    "        \n",
    "        self.saver = tf.train.Saver() # saver\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _build_a(self, s, scope, trainable): # policy\n",
    "        with tf.variable_scope(scope):\n",
    "            net = tf.layers.dense(s, 200, activation=tf.nn.relu, name='l1', trainable=trainable)\n",
    "            net = tf.layers.dense(net, 100, activation=tf.nn.relu, name='l2', trainable=trainable)\n",
    "            net = tf.layers.dense(net, 30, activation=tf.nn.relu, name='l3', trainable=trainable)\n",
    "            a = tf.layers.dense(net, self.action_size, activation=tf.nn.tanh, name='a', trainable=trainable)\n",
    "            self.actor_model_set = True\n",
    "            return a * (self.action_high-self.action_low)/2 + (self.action_high+self.action_low)/2\n",
    "    \n",
    "    def _build_c(self, s, a, scope, trainable): # advantage value\n",
    "        with tf.variable_scope(scope):\n",
    "            n_l1 = 200\n",
    "            w1_s = tf.get_variable('w1_s', [self.state_size, n_l1], trainable=trainable)\n",
    "            w1_a = tf.get_variable('w1_a', [self.action_size, n_l1], trainable=trainable)\n",
    "            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)\n",
    "            net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "            net = tf.layers.dense(net, 100, activation=tf.nn.relu, name='l2', trainable=trainable)\n",
    "            net = tf.layers.dense(net, 30, activation=tf.nn.relu, name='l3', trainable=trainable)\n",
    "            self.critic_model_set = True\n",
    "            return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if self.priority_alpha > 0: # prioritised\n",
    "            self.memory.add((state, action, reward, next_state, done), \n",
    "                            self.error(state, action, reward, next_state))\n",
    "        else: # non prioritised, every memory has priority 1\n",
    "            self.memory.add((state, action, reward, next_state, done), 1)\n",
    "            \n",
    "    def error(self, state, action, reward, next_state):\n",
    "        return self.sess.run(self.td_error, {self.S: state, self.a: [action], \n",
    "                                             self.R: [[reward]], self.S_: next_state})\n",
    "        \n",
    "    def choose_action(self, state, variance, low, high): # normal distribution\n",
    "        assert self.actor_model_set, 'actor model not set!'\n",
    "        action = self.sess.run(self.a, {self.S: state})[0]\n",
    "        return np.clip(np.random.normal(action, variance), low, high)\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        assert self.actor_model_set, 'model not set!'\n",
    "        assert self.critic_model_set, 'critic model not set!'\n",
    "        minibatch = self.memory.sample(batch_size)\n",
    "        idxs, states, actions, rewards, next_states = [], [], [], [], []\n",
    "        for idx, (state, action, reward, next_state, _) in minibatch:\n",
    "            idxs+=[idx]\n",
    "            states+=[state]\n",
    "            actions+=[action]\n",
    "            rewards+=[reward]\n",
    "            next_states+=[next_state]\n",
    "        \n",
    "        self.sess.run(self.atrain, {self.S: np.vstack(states)})\n",
    "        self.sess.run(self.ctrain, {self.S: np.vstack(states), self.a: np.vstack(actions),\n",
    "                                    self.R: np.vstack(rewards), self.S_: np.vstack(next_states)})\n",
    "        self.sess.run(self.soft_replace) # update the weights\n",
    "        \n",
    "        if self.priority_alpha > 0: # prioritised, update\n",
    "            errors = self.sess.run(self.td_error_element_wise, {self.S: np.vstack(states), self.a: np.vstack(actions),\n",
    "                                                                self.R: np.vstack(rewards), self.S_: np.vstack(next_states)})\n",
    "            for i in range(len(idxs)):\n",
    "                self.memory.update(idxs[i], errors[i])\n",
    "        \n",
    "        self.actor_learning_rate *= self.learning_rate_decay\n",
    "        self.critic_learning_rate *= self.learning_rate_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import sklearn.preprocessing\n",
    "import sklearn.pipeline\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from numpy import newaxis\n",
    "\n",
    "METHOD = [\n",
    "    dict(name='kl_pen', kl_target=0.01, lam=0.5),   # KL penalty\n",
    "    dict(name='clip', epsilon=0.2),                 # Clipped surrogate objective, find this is better\n",
    "][1]        # choose the method for optimization\n",
    "\n",
    "class PPOTest:\n",
    "    def __init__(self, env:gym.Env, n_actions, n_features, action_low, action_high, featurize=False, reward_decay=0.95,\n",
    "                 actor_learning_rate=0.01, critic_learning_rate=0.01, learning_rate_decay=0.9995,\n",
    "                 tau=1.0):\n",
    "        self.env = env\n",
    "        self.state_size = n_features\n",
    "        self.action_size = n_actions\n",
    "        self.action_low = action_low\n",
    "        self.action_high = action_high\n",
    "        self.gamma = reward_decay   # discount rate\n",
    "        self.actor_model_set = True\n",
    "        self.critic_model_set = True\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.critic_learning_rate = critic_learning_rate # often larger than actor_learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.tau = tau # soft update\n",
    "        self.batch_size = 32\n",
    "        self.memory = [] # store (s, a, r)\n",
    "        self.featurize = featurize\n",
    "        if featurize:\n",
    "            self._init_featurizer()\n",
    "        self._construct_nets()\n",
    "        \n",
    "    def _construct_nets(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.tfs = tf.placeholder(tf.float32, [None, self.state_size], 'state')\n",
    "\n",
    "        # critic\n",
    "        with tf.variable_scope('critic'):\n",
    "            net = tf.layers.dense(self.tfs, 400, tf.nn.relu)\n",
    "            net = tf.layers.dense(net, 200, tf.nn.relu)\n",
    "#             l3 = tf.layers.dense(l2, 30, tf.nn.relu)\n",
    "            self.v = tf.layers.dense(net, 1)\n",
    "            self.tfdc_r = tf.placeholder(tf.float32, [None, 1], 'discounted_r')\n",
    "            self.advantage = self.tfdc_r - self.v\n",
    "            self.closs = tf.reduce_mean(tf.square(self.advantage))\n",
    "            self.ctrain_op = tf.train.AdamOptimizer(self.critic_learning_rate).minimize(self.closs)\n",
    "\n",
    "        # actor\n",
    "        pi, pi_params = self._build_anet('pi', trainable=True)\n",
    "        oldpi, oldpi_params = self._build_anet('oldpi', trainable=False)\n",
    "        with tf.variable_scope('sample_action'):\n",
    "            self.sample_op = tf.squeeze(pi.sample(1), axis=0)       # choosing action\n",
    "        with tf.variable_scope('update_oldpi'):\n",
    "            self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n",
    "\n",
    "        self.tfa = tf.placeholder(tf.float32, [None, self.action_size], 'action')\n",
    "        self.tfadv = tf.placeholder(tf.float32, [None, 1], 'advantage')\n",
    "        with tf.variable_scope('loss'):\n",
    "            with tf.variable_scope('surrogate'):\n",
    "#                 ratio = tf.exp(pi.log_prob(self.tfa) - oldpi.log_prob(self.tfa))\n",
    "                ratio = pi.prob(self.tfa) / (oldpi.prob(self.tfa)+1e-10)\n",
    "                surr = ratio * self.tfadv\n",
    "            if METHOD['name'] == 'kl_pen':\n",
    "                self.tflam = tf.placeholder(tf.float32, None, 'lambda')\n",
    "                kl = tf.distributions.kl_divergence(oldpi, pi)\n",
    "                self.kl_mean = tf.reduce_mean(kl)\n",
    "                self.aloss = -(tf.reduce_mean(surr - self.tflam * kl))\n",
    "            else:   # clipping method, find this is better\n",
    "                self.aloss = -tf.reduce_mean(tf.minimum(\n",
    "                    surr,\n",
    "                    tf.clip_by_value(ratio, 1.-METHOD['epsilon'], 1.+METHOD['epsilon'])*self.tfadv))\n",
    "\n",
    "        with tf.variable_scope('atrain'):\n",
    "            self.atrain_op = tf.train.AdamOptimizer(self.actor_learning_rate).minimize(self.aloss)\n",
    "            \n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build_anet(self, name, trainable):\n",
    "        with tf.variable_scope(name):\n",
    "            net = tf.layers.dense(self.tfs, 400, tf.nn.relu, trainable=trainable)\n",
    "            net = tf.layers.dense(net, 200, tf.nn.relu, trainable=trainable)\n",
    "            net = tf.layers.dense(net, 100, tf.nn.relu, trainable=trainable)\n",
    "            mu = max(np.abs(self.action_low), np.abs(self.action_high)) * tf.layers.dense(net, self.action_size, tf.nn.tanh, trainable=trainable)\n",
    "            sigma = tf.layers.dense(net, self.action_size, tf.nn.softplus, trainable=trainable)\n",
    "            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma+1e-5)\n",
    "        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n",
    "        return norm_dist, params\n",
    "    \n",
    "    def choose_action(self, state): # normal distribution\n",
    "        assert self.actor_model_set, 'actor model not set!'\n",
    "        a = self.sess.run(self.sample_op, {self.tfs: state})[0]\n",
    "        return np.clip(a, self.action_low, self.action_high)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory += [[state[0], action, reward, next_state[0]]]\n",
    "    \n",
    "    def replay(self):\n",
    "        assert self.actor_model_set, 'model not set!'\n",
    "        assert self.critic_model_set, 'critic model not set!'\n",
    "        memory = np.vstack(self.memory)\n",
    "        states = np.vstack(memory[:,0])\n",
    "        actions = np.vstack(memory[:,1])\n",
    "        rewards = memory[:,2]\n",
    "        last_next_state = memory[:,3][-1]\n",
    "        \n",
    "        discounted_ep_rs = np.zeros_like(rewards)\n",
    "        running_add = self.sess.run(self.v, {self.tfs: [last_next_state]})[0]\n",
    "        for t in reversed(range(0, len(memory))):\n",
    "            running_add = running_add * self.gamma + rewards[t]\n",
    "            discounted_ep_rs[t] = running_add\n",
    "        \n",
    "        self.sess.run(self.update_oldpi_op)\n",
    "        adv = self.sess.run(self.advantage, {self.tfs: states, self.tfdc_r: discounted_ep_rs[:, newaxis]})\n",
    "        [self.sess.run(self.atrain_op, {self.tfs: states, self.tfa: actions, self.tfadv: adv}) for _ in range(10)]\n",
    "        [self.sess.run(self.ctrain_op, {self.tfs: states, self.tfdc_r: discounted_ep_rs[:, newaxis]}) for _ in range(10)]\n",
    "        \n",
    "        self.actor_learning_rate *= self.learning_rate_decay\n",
    "        self.critic_learning_rate *= self.learning_rate_decay\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 8\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 2\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "env_name = \"balance_broom\"\n",
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state looks like: \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "train_mode = False\n",
    "    \n",
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "    \n",
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.states[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = DDPGAgent(env,\n",
    "            n_actions=2,\n",
    "            n_features=8,\n",
    "            featurize=False, \n",
    "            action_high=1,\n",
    "            action_low=-1,\n",
    "            actor_learning_rate=0.0001,\n",
    "            critic_learning_rate=0.0002,\n",
    "            priority_alpha=0\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOTest(env,\n",
    "                n_actions=2,\n",
    "                n_features=8,\n",
    "                featurize=False, \n",
    "                action_high=1,\n",
    "                action_low=-1,\n",
    "                actor_learning_rate=0.0001,\n",
    "                critic_learning_rate=0.0002\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 866 rewards: 9146.84 explore var: 0.795 rewards: 70.27 explore var: 3.00 134 rewards: 95.35 explore var: 3.00 155 rewards: 28.06 explore var: 3.00rewards: 26.75 explore var: 2.99explore var: 2.98 229 rewards: 28.73 explore var: 2.94 277 rewards: 24.02 explore var: 2.91 289 rewards: 22.40 explore var: 2.90explore var: 0.85 738 rewards: 27.94 explore var: 0.85 742 rewards: 22.73 explore var: 0.85778 rewards: 27.02 explore var: 0.84\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-fbde9bf4a81c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_low\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_high\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0menv_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdefault_brain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-91cec228eead>\u001b[0m in \u001b[0;36mchoose_action\u001b[1;34m(self, state, variance, low, high)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# normal distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_model_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'actor model not set!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kwea123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kwea123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kwea123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kwea123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kwea123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#DDPG\n",
    "n_episodes = 2000\n",
    "\n",
    "# agent.saver.restore(agent.sess, \"model/model.ckpt\")\n",
    "rewards = []\n",
    "for i_episode in range(n_episodes):\n",
    "    env_info = agent.env.reset(train_mode=True)[default_brain]\n",
    "    state = env_info.states\n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action(state, agent.variance, agent.action_low, agent.action_high)\n",
    "        env_info = agent.env.step(action)[default_brain]\n",
    "        next_state = env_info.states\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        r += reward\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        # learn when memory is full, every BATCH steps\n",
    "        if len(agent.memory) == agent.memory_size:\n",
    "            agent.variance *= 0.999995\n",
    "            agent.replay(agent.batch_size)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, \"explore var: %.2f\" % agent.variance, end=\"\\r\")\n",
    "            rewards += [r]\n",
    "            break\n",
    "print(\"\\n\")\n",
    "print(\"finished learning!\")\n",
    "# agent.saver.save(agent.sess, \"model/model3g_ddpg.ckpt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2000 rewards: 345.81ewards: 84.30 60 rewards: 66.85 88 rewards: 71.24 228 rewards: 106.42 246 rewards: 83.40 347 rewards: 80.50 435 rewards: 75.69 499 rewards: 37.25 541 rewards: 80.60 545 rewards: 77.30 557 rewards: 63.04 610 rewards: 70.11 653 rewards: 82.74 1109 rewards: 73.89 1118 rewards: 63.66 1228 rewards: 66.78 1337 rewards: 62.79 1380 rewards: 30.56 1458 rewards: 64.32 1584 rewards: 66.48 rewards: 83.84 1753 rewards: 83.85 1802 rewards: 77.12 1821 rewards: 75.20 1911 rewards: 93.09 1943 rewards: 85.40 1948 rewards: 81.74\n",
      "\n",
      "finished learning!\n"
     ]
    }
   ],
   "source": [
    "# PPO\n",
    "n_episodes = 2000\n",
    "\n",
    "# agent.saver.restore(agent.sess, \"model/model3g_ppo3.ckpt\")\n",
    "rewards = []\n",
    "for i_episode in range(n_episodes):\n",
    "    env_info = agent.env.reset(train_mode=True)[default_brain]\n",
    "    state = env_info.states\n",
    "    r = 0\n",
    "    count = 0\n",
    "    while count<4500:\n",
    "        action = agent.choose_action(state)\n",
    "        env_info = agent.env.step(action)[default_brain]\n",
    "        next_state = env_info.states\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        r += reward\n",
    "        agent.remember(state, action, reward, next_state)\n",
    "        # learn when memory is full, every BATCH steps\n",
    "        if len(agent.memory) == agent.batch_size or done:\n",
    "            agent.replay()\n",
    "        state = next_state\n",
    "        count+=1\n",
    "        if done or count==4500:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            rewards += [r]\n",
    "            break\n",
    "print(\"\\n\")\n",
    "print(\"finished learning!\")\n",
    "agent.saver.save(agent.sess, \"model/model3g_ppo2.ckpt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC63QTudQrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x219bf74cc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model3g_ddpg.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-20 16:31:47,576] Restoring parameters from model/model3g_ddpg.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8 rewards: 6784.894\r"
     ]
    }
   ],
   "source": [
    "#ddpg\n",
    "n_episodes = 10\n",
    "\n",
    "test_rewards = []\n",
    "agent.saver.restore(agent.sess, \"model/model3g_ddpg.ckpt\")\n",
    "for i_episode in range(n_episodes):\n",
    "    env_info = agent.env.reset(train_mode=False)[default_brain]\n",
    "    state = env_info.states\n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action(state, 0, agent.action_low, agent.action_high)\n",
    "        env_info = agent.env.step(action)[default_brain]\n",
    "        next_state = env_info.states\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        r += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            test_rewards += [r]\n",
    "            break\n",
    "print(\"\\n\")\n",
    "print(\"finished testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 20 rewards: 160.62\n",
      "\n",
      "finished testing!\n"
     ]
    }
   ],
   "source": [
    "#ppo\n",
    "n_episodes = 10\n",
    "\n",
    "test_rewards = []\n",
    "# agent.saver.restore(agent.sess, \"model/model3g_ppo3.ckpt\")\n",
    "for i_episode in range(n_episodes):\n",
    "    env_info = agent.env.reset(train_mode=False)[default_brain]\n",
    "    state = env_info.states\n",
    "    r = 0\n",
    "    while True:\n",
    "        action = agent.choose_action(state)\n",
    "        env_info = agent.env.step(action)[default_brain]\n",
    "        next_state = env_info.states\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        r += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode:\", i_episode+1, \"rewards: %.2f\" % r, end=\"\\r\")\n",
    "            test_rewards += [r]\n",
    "            break\n",
    "print(\"\\n\")\n",
    "print(\"finished testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDX+Dzd7Jv6ajfm4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x245790109b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
